import os
import json
import time
import re
from difflib import SequenceMatcher
from pathlib import Path
import requests
from bs4 import BeautifulSoup
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# –ó–∞–≥—Ä—É–∑–∫–∞ .env —Ñ–∞–π–ª–∞
def load_env_file():
    env_path = Path(__file__).parent.parent / '.env'
    if env_path.exists():
        with open(env_path) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ.setdefault(key.strip(), value.strip())

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_env_file()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
DUPLICATE_THRESHOLD = 0.8
RUSSIAN_TEXT_THRESHOLD = 0.8
MAX_TELEGRAM_LENGTH = 4000
INPUT_FILE = "news_raw.json"
OUTPUT_FILE = "result_news.json"
IMAGES_DIR = "processed_images"

os.makedirs(IMAGES_DIR, exist_ok=True)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π
translation_pipe = None
summarization_pipe = None
device = None

def init_models():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π Hugging Face"""
    global translation_pipe, summarization_pipe, device

    print("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π Hugging Face...")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (CPU –¥–ª—è railway.app)
    device = 0 if torch.cuda.is_available() else -1
    device_name = "GPU" if device == 0 else "CPU"
    print(f"   üì± –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device_name}")

    try:
        # –ú–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ (Helsinki-NLP opus-mt)
        # –ú–∞–ª–µ–Ω—å–∫–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–∞–Ω—Å–∫–∏–π -> –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
        print("   üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ (es->en)...")
        translation_model_name = "Helsinki-NLP/opus-mt-es-en"
        translation_pipe = pipeline(
            "translation",
            model=translation_model_name,
            device=device,
            max_length=512
        )

        # –ú–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π -> —Ä—É—Å—Å–∫–∏–π
        print("   üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ (en->ru)...")
        translation_en_ru_name = "Helsinki-NLP/opus-mt-en-ru"
        translation_en_ru_pipe = pipeline(
            "translation",
            model=translation_en_ru_name,
            device=device,
            max_length=512
        )

        # –ú–æ–¥–µ–ª—å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è)
        print("   üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏...")
        summarization_model_name = "facebook/bart-large-cnn"
        summarization_pipe = pipeline(
            "summarization",
            model=summarization_model_name,
            device=device
        )

        print("   ‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        return translation_pipe, translation_en_ru_pipe, summarization_pipe

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
        raise

def is_duplicate(title, seen_titles):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–º"""
    for seen in seen_titles:
        if SequenceMatcher(None, title.lower(), seen.lower()).ratio() > DUPLICATE_THRESHOLD:
            return True
    return False

def is_russian_text(text, threshold=RUSSIAN_TEXT_THRESHOLD):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∏–Ω–∏–º—É–º threshold% —Ä—É—Å—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if not text or not text.strip():
        return False

    russian_chars = set('–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø')
    letters = [char for char in text if char.isalpha()]

    if not letters:
        return False

    russian_count = sum(1 for char in letters if char in russian_chars)
    russian_ratio = russian_count / len(letters)

    return russian_ratio >= threshold

def has_hashtags(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ö—ç—à—Ç–µ–≥–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
    if not text or not text.strip():
        return False

    hashtags = re.findall(r'#\w+', text)
    return len(hashtags) >= 2

def is_telegram_compatible(title, description, link):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –ª–∏–º–∏—Ç–∞–º–∏ Telegram"""
    formatted_text = f"üì∞ *{title}*\n\n{description}\n\nüîó [–°—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫]({link})"
    return len(formatted_text) <= MAX_TELEGRAM_LENGTH

def fetch_article_content(url):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ –ø–æ —Å—Å—ã–ª–∫–µ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
            element.decompose()

        article_text = ""

        article = soup.find('article')
        if article:
            paragraphs = article.find_all('p')
            article_text = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])

        if not article_text:
            content_divs = soup.find_all(['div'], class_=lambda x: x and any(
                word in str(x).lower() for word in ['article', 'content', 'story', 'post']
            ))
            for div in content_divs:
                paragraphs = div.find_all('p')
                text = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
                if len(text) > len(article_text):
                    article_text = text

        if not article_text or len(article_text) < 200:
            paragraphs = soup.find_all('p')
            article_text = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])

        return article_text[:8000] if article_text else ""

    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
        return ""

def split_text_into_chunks(text, max_length=400):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º"""
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= max_length:
            current_chunk += sentence + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + " "

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

def translate_and_summarize(text, is_title=False, translation_es_en_pipe=None,
                           translation_en_ru_pipe=None, summarization_pipe=None,
                           max_retries=3):
    """
    –ü–µ—Ä–µ–≤–æ–¥–∏—Ç –∏ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é HuggingFace Transformers.
    –°—Ö–µ–º–∞: –ò—Å–ø–∞–Ω—Å–∫–∏–π -> –ê–Ω–≥–ª–∏–π—Å–∫–∏–π -> –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è -> –†—É—Å—Å–∫–∏–π
    """
    for attempt in range(max_retries):
        try:
            if is_title:
                # –î–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞–ø—Ä—è–º—É—é
                print(f"   üîÑ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (es->en->ru)...")

                # –®–∞–≥ 1: –ò—Å–ø–∞–Ω—Å–∫–∏–π -> –ê–Ω–≥–ª–∏–π—Å–∫–∏–π
                en_result = translation_es_en_pipe(text, max_length=100)
                en_text = en_result[0]['translation_text']

                time.sleep(0.5)

                # –®–∞–≥ 2: –ê–Ω–≥–ª–∏–π—Å–∫–∏–π -> –†—É—Å—Å–∫–∏–π
                ru_result = translation_en_ru_pipe(en_text, max_length=100)
                translated_text = ru_result[0]['translation_text']

                return translated_text.strip()
            else:
                # –î–ª—è —Ç–µ–∫—Å—Ç–∞: –ø–µ—Ä–µ–≤–æ–¥–∏–º, —Å—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º, –ø–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞ —Ä—É—Å—Å–∫–∏–π
                print(f"   üîÑ –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ (es->en)...")

                # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
                chunks = split_text_into_chunks(text, max_length=400)
                en_chunks = []

                for i, chunk in enumerate(chunks):
                    if i > 0:
                        time.sleep(0.5)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    result = translation_es_en_pipe(chunk, max_length=512)
                    en_chunks.append(result[0]['translation_text'])

                en_text = " ".join(en_chunks)

                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
                en_text = en_text[:3000]

                print(f"   üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...")
                time.sleep(0.5)

                # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º
                summary = summarization_pipe(
                    en_text,
                    max_length=300,
                    min_length=100,
                    do_sample=False
                )
                summarized_text = summary[0]['summary_text']

                print(f"   üîÑ –ü–µ—Ä–µ–≤–æ–¥ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (en->ru)...")
                time.sleep(0.5)

                # –ü–µ—Ä–µ–≤–æ–¥–∏–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ —Ä—É—Å—Å–∫–∏–π –ø–æ —á–∞—Å—Ç—è–º
                summary_chunks = split_text_into_chunks(summarized_text, max_length=400)
                ru_chunks = []

                for i, chunk in enumerate(summary_chunks):
                    if i > 0:
                        time.sleep(0.5)
                    result = translation_en_ru_pipe(chunk, max_length=512)
                    ru_chunks.append(result[0]['translation_text'])

                final_text = " ".join(ru_chunks)

                # –î–æ–±–∞–≤–ª—è–µ–º —Ö—ç—à—Ç–µ–≥–∏ (–∏–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
                hashtags = generate_hashtags(text)
                final_text = f"{final_text}\n\n{hashtags}"

                return final_text.strip()

        except Exception as e:
            error_msg = str(e)
            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}/{max_retries}: {error_msg[:100]}")

            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 5
                print(f"   ‚è≥ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {wait_time} —Å–µ–∫—É–Ω–¥...")
                time.sleep(wait_time)
            else:
                raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")

    return "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç"

def generate_hashtags(text):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö—ç—à—Ç–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥: –±–µ—Ä—ë–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
    keywords = ['Espa√±a', '–ò—Å–ø–∞–Ω–∏—è', 'Valencia', 'Madrid', 'Barcelona',
                'Gobierno', 'Econom√≠a', 'Pol√≠tica', 'Sociedad']

    found_tags = []
    text_lower = text.lower()

    if 'espa√±a' in text_lower or 'spanish' in text_lower:
        found_tags.append('#–ò—Å–ø–∞–Ω–∏—è')
    if 'valencia' in text_lower:
        found_tags.append('#–í–∞–ª–µ–Ω—Å–∏—è')
    if 'madrid' in text_lower:
        found_tags.append('#–ú–∞–¥—Ä–∏–¥')
    if 'gobierno' in text_lower or 'pol√≠tica' in text_lower or 'government' in text_lower:
        found_tags.append('#–ü–æ–ª–∏—Ç–∏–∫–∞')
    if 'econom√≠a' in text_lower or 'economy' in text_lower:
        found_tags.append('#–≠–∫–æ–Ω–æ–º–∏–∫–∞')

    # –ï—Å–ª–∏ –º–µ–Ω—å—à–µ 3 —Ö—ç—à—Ç–µ–≥–æ–≤, –¥–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ
    if len(found_tags) < 3:
        default_tags = ['#–ñ–∏–∑–Ω—å–í–ò—Å–ø–∞–Ω–∏–∏', '#–ù–æ–≤–æ—Å—Ç–∏–ò—Å–ø–∞–Ω–∏–∏', '#Espa√±a']
        for tag in default_tags:
            if tag not in found_tags:
                found_tags.append(tag)
            if len(found_tags) >= 3:
                break

    return ' '.join(found_tags[:4])

def main():
    input_path = Path(__file__).parent.parent / INPUT_FILE
    output_path = Path(__file__).parent.parent / OUTPUT_FILE

    if not input_path.exists():
        print(f"‚ùå –§–∞–π–ª {INPUT_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ fetch_news.py")
        return

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
    translation_es_en, translation_en_ru, summarization = init_models()

    print(f"\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {INPUT_FILE}...")
    with open(input_path, 'r', encoding='utf-8') as f:
        news_items = json.load(f)

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π")

    processed_news = []
    seen_titles = []

    for idx, news in enumerate(news_items, 1):
        title = news.get("title", "")
        description = news.get("description", "")

        print(f"\n[{idx}/{len(news_items)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {title[:50]}...")

        if is_duplicate(title, seen_titles):
            print("   ‚ö†Ô∏è  –î—É–±–ª–∏–∫–∞—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue

        seen_titles.append(title)

        try:
            print(f"   ü§ñ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞...")
            rewritten_title = translate_and_summarize(
                title,
                is_title=True,
                translation_es_en_pipe=translation_es_en,
                translation_en_ru_pipe=translation_en_ru,
                summarization_pipe=summarization
            )

            time.sleep(2)

            link = news.get("link", "")
            print(f"   üîó –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏...")
            article_content = fetch_article_content(link)

            if article_content:
                text_to_process = f"{title}. {article_content}"
                print(f"   üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(article_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                text_to_process = f"{title}. {description}"
                print(f"   ‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º description")

            print(f"   ü§ñ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...")
            rewritten_text = translate_and_summarize(
                text_to_process,
                translation_es_en_pipe=translation_es_en,
                translation_en_ru_pipe=translation_en_ru,
                summarization_pipe=summarization
            )

            time.sleep(2)

            if not rewritten_title or not rewritten_title.strip():
                print(f"   ‚ö†Ô∏è  –ü—É—Å—Ç–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            if not rewritten_text or not rewritten_text.strip():
                print(f"   ‚ö†Ô∏è  –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            if not is_russian_text(rewritten_title):
                print(f"   ‚ö†Ô∏è  –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            if not is_russian_text(rewritten_text):
                print(f"   ‚ö†Ô∏è  –¢–µ–∫—Å—Ç –Ω–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            if not has_hashtags(rewritten_text):
                print(f"   ‚ö†Ô∏è  –ù–µ—Ç —Ö—ç—à—Ç–µ–≥–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            if not is_telegram_compatible(rewritten_title, rewritten_text, link):
                print(f"   ‚ö†Ô∏è  –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç Telegram, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            print(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ")

            processed_news.append({
                "title": rewritten_title,
                "link": news.get("link", ""),
                "description": rewritten_text,
                "published": news.get("published", ""),
                "author": news.get("author", ""),
                "categories": news.get("categories", []),
                "image": news.get("image")
            })
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            continue

    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –≤ {OUTPUT_FILE}...")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(processed_news, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(processed_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

if __name__ == "__main__":
    main()
