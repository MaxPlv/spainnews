import os
import json
import time
import re
from difflib import SequenceMatcher
from pathlib import Path
import requests
from bs4 import BeautifulSoup

# –ó–∞–≥—Ä—É–∑–∫–∞ .env —Ñ–∞–π–ª–∞
def load_env_file():
    env_path = Path(__file__).parent.parent / '.env'
    if env_path.exists():
        with open(env_path) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ.setdefault(key.strip(), value.strip())

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_env_file()

# Hugging Face API Token (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π)
HF_API_TOKEN = os.getenv("HUGGING_FACE_API_TOKEN")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
DUPLICATE_THRESHOLD = 0.8
RUSSIAN_TEXT_THRESHOLD = 0.8
MAX_TELEGRAM_LENGTH = 4000
INPUT_FILE = "news_raw.json"
OUTPUT_FILE = "result_news.json"
IMAGES_DIR = "processed_images"

os.makedirs(IMAGES_DIR, exist_ok=True)

# Hugging Face API endpoints
HF_API_BASE = "https://api-inference.huggingface.co/models/"
TRANSLATION_ES_EN_MODEL = "Helsinki-NLP/opus-mt-es-en"
TRANSLATION_EN_RU_MODEL = "Helsinki-NLP/opus-mt-en-ru"
SUMMARIZATION_MODEL = "facebook/bart-large-cnn"

def query_huggingface_api(model_name, payload, max_retries=3):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ Hugging Face Inference API"""
    api_url = f"{HF_API_BASE}{model_name}"
    headers = {}

    if HF_API_TOKEN:
        headers["Authorization"] = f"Bearer {HF_API_TOKEN}"

    for attempt in range(max_retries):
        try:
            response = requests.post(api_url, headers=headers, json=payload, timeout=30)

            if response.status_code == 503:
                # –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è
                print(f"   ‚è≥ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è, –æ–∂–∏–¥–∞–Ω–∏–µ {10 * (attempt + 1)} —Å–µ–∫...")
                time.sleep(10 * (attempt + 1))
                continue

            response.raise_for_status()
            return response.json()

        except requests.exceptions.RequestException as e:
            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ API (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(5 * (attempt + 1))
            else:
                raise

    return None

def is_duplicate(title, seen_titles):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–º"""
    for seen in seen_titles:
        if SequenceMatcher(None, title.lower(), seen.lower()).ratio() > DUPLICATE_THRESHOLD:
            return True
    return False

def is_russian_text(text, threshold=RUSSIAN_TEXT_THRESHOLD):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∏–Ω–∏–º—É–º threshold% —Ä—É—Å—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if not text or not text.strip():
        return False

    russian_chars = set('–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø')
    letters = [char for char in text if char.isalpha()]

    if not letters:
        return False

    russian_count = sum(1 for char in letters if char in russian_chars)
    russian_ratio = russian_count / len(letters)

    return russian_ratio >= threshold

def has_hashtags(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ö—ç—à—Ç–µ–≥–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
    if not text or not text.strip():
        return False

    hashtags = re.findall(r'#\w+', text)
    return len(hashtags) >= 2

def is_telegram_compatible(title, description, link):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –ª–∏–º–∏—Ç–∞–º–∏ Telegram"""
    formatted_text = f"üì∞ *{title}*\n\n{description}\n\nüîó [–°—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫]({link})"
    return len(formatted_text) <= MAX_TELEGRAM_LENGTH

def fetch_article_content(url):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ –ø–æ —Å—Å—ã–ª–∫–µ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
            element.decompose()

        article_text = ""

        article = soup.find('article')
        if article:
            paragraphs = article.find_all('p')
            article_text = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])

        if not article_text:
            content_divs = soup.find_all(['div'], class_=lambda x: x and any(
                word in str(x).lower() for word in ['article', 'content', 'story', 'post']
            ))
            for div in content_divs:
                paragraphs = div.find_all('p')
                text = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
                if len(text) > len(article_text):
                    article_text = text

        if not article_text or len(article_text) < 200:
            paragraphs = soup.find_all('p')
            article_text = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])

        return article_text[:5000] if article_text else ""

    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
        return ""

def split_text_into_chunks(text, max_length=400):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º"""
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= max_length:
            current_chunk += sentence + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + " "

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

def translate_es_to_en(text):
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Å –∏—Å–ø–∞–Ω—Å–∫–æ–≥–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π"""
    print(f"   üîÑ –ü–µ—Ä–µ–≤–æ–¥ es‚Üíen...")
    result = query_huggingface_api(
        TRANSLATION_ES_EN_MODEL,
        {"inputs": text}
    )

    if result and isinstance(result, list) and len(result) > 0:
        return result[0].get('translation_text', '')
    return ""

def translate_en_to_ru(text):
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π"""
    print(f"   üîÑ –ü–µ—Ä–µ–≤–æ–¥ en‚Üíru...")

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω—ã–π
    chunks = split_text_into_chunks(text, max_length=400)
    translated_chunks = []

    for chunk in chunks:
        result = query_huggingface_api(
            TRANSLATION_EN_RU_MODEL,
            {"inputs": chunk}
        )

        if result and isinstance(result, list) and len(result) > 0:
            translated_chunks.append(result[0].get('translation_text', ''))
        time.sleep(1)

    return " ".join(translated_chunks)

def summarize_text(text):
    """–°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º"""
    print(f"   üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è...")

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è API
    text = text[:2000]

    result = query_huggingface_api(
        SUMMARIZATION_MODEL,
        {
            "inputs": text,
            "parameters": {
                "max_length": 300,
                "min_length": 100,
                "do_sample": False
            }
        }
    )

    if result and isinstance(result, list) and len(result) > 0:
        return result[0].get('summary_text', '')
    return ""

def translate_and_summarize(text, is_title=False):
    """
    –ü–µ—Ä–µ–≤–æ–¥–∏—Ç –∏ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Hugging Face API.
    –°—Ö–µ–º–∞: –ò—Å–ø–∞–Ω—Å–∫–∏–π -> –ê–Ω–≥–ª–∏–π—Å–∫–∏–π -> –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è -> –†—É—Å—Å–∫–∏–π
    """
    try:
        if is_title:
            # –î–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º
            en_text = translate_es_to_en(text)
            time.sleep(1)

            ru_text = translate_en_to_ru(en_text)
            return ru_text.strip()
        else:
            # –î–ª—è —Ç–µ–∫—Å—Ç–∞: –ø–µ—Ä–µ–≤–æ–¥–∏–º, —Å—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º, –ø–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞ —Ä—É—Å—Å–∫–∏–π
            chunks = split_text_into_chunks(text, max_length=400)
            en_chunks = []

            for i, chunk in enumerate(chunks[:10]):  # –ú–∞–∫—Å–∏–º—É–º 10 —á–∞—Å—Ç–µ–π
                if i > 0:
                    time.sleep(1)
                en_text = translate_es_to_en(chunk)
                if en_text:
                    en_chunks.append(en_text)

            en_full_text = " ".join(en_chunks)
            time.sleep(2)

            # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
            summary = summarize_text(en_full_text)
            if not summary:
                # –ï—Å–ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞
                summary = en_full_text[:500]

            time.sleep(2)

            # –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–∏–π
            ru_text = translate_en_to_ru(summary)

            # –î–æ–±–∞–≤–ª—è–µ–º —Ö—ç—à—Ç–µ–≥–∏
            hashtags = generate_hashtags(text)
            final_text = f"{ru_text}\n\n{hashtags}"

            return final_text.strip()

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
        raise

def generate_hashtags(text):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö—ç—à—Ç–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    found_tags = []
    text_lower = text.lower()

    if 'espa√±a' in text_lower or 'spanish' in text_lower:
        found_tags.append('#–ò—Å–ø–∞–Ω–∏—è')
    if 'valencia' in text_lower:
        found_tags.append('#–í–∞–ª–µ–Ω—Å–∏—è')
    if 'madrid' in text_lower:
        found_tags.append('#–ú–∞–¥—Ä–∏–¥')
    if 'barcelona' in text_lower:
        found_tags.append('#–ë–∞—Ä—Å–µ–ª–æ–Ω–∞')
    if 'gobierno' in text_lower or 'pol√≠tica' in text_lower or 'government' in text_lower:
        found_tags.append('#–ü–æ–ª–∏—Ç–∏–∫–∞')
    if 'econom√≠a' in text_lower or 'economy' in text_lower:
        found_tags.append('#–≠–∫–æ–Ω–æ–º–∏–∫–∞')

    # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ —Ç–µ–≥–∏
    if len(found_tags) < 3:
        default_tags = ['#–ñ–∏–∑–Ω—å–í–ò—Å–ø–∞–Ω–∏–∏', '#–ù–æ–≤–æ—Å—Ç–∏–ò—Å–ø–∞–Ω–∏–∏', '#Espa√±a']
        for tag in default_tags:
            if tag not in found_tags:
                found_tags.append(tag)
            if len(found_tags) >= 3:
                break

    return ' '.join(found_tags[:4])

def main():
    input_path = Path(__file__).parent.parent / INPUT_FILE
    output_path = Path(__file__).parent.parent / OUTPUT_FILE

    if not input_path.exists():
        print(f"‚ùå –§–∞–π–ª {INPUT_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ fetch_news.py")
        return

    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {INPUT_FILE}...")
    with open(input_path, 'r', encoding='utf-8') as f:
        news_items = json.load(f)

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π")

    if HF_API_TOKEN:
        print(f"üîë –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Hugging Face API Token")
    else:
        print(f"‚ö†Ô∏è  Hugging Face API Token –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω–æ–µ API (–º–æ–≥—É—Ç –±—ã—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)")

    processed_news = []
    seen_titles = []

    for idx, news in enumerate(news_items, 1):
        title = news.get("title", "")
        description = news.get("description", "")

        print(f"\n[{idx}/{len(news_items)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {title[:50]}...")

        if is_duplicate(title, seen_titles):
            print("   ‚ö†Ô∏è  –î—É–±–ª–∏–∫–∞—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue

        seen_titles.append(title)

        try:
            print(f"   ü§ñ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞...")
            rewritten_title = translate_and_summarize(title, is_title=True)
            time.sleep(3)

            link = news.get("link", "")
            print(f"   üîó –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏...")
            article_content = fetch_article_content(link)

            if article_content:
                text_to_process = f"{title}. {article_content}"
                print(f"   üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(article_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                text_to_process = f"{title}. {description}"
                print(f"   ‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º description")

            print(f"   ü§ñ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...")
            rewritten_text = translate_and_summarize(text_to_process)
            time.sleep(3)

            if not rewritten_title or not rewritten_title.strip():
                print(f"   ‚ö†Ô∏è  –ü—É—Å—Ç–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            if not rewritten_text or not rewritten_text.strip():
                print(f"   ‚ö†Ô∏è  –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            if not is_russian_text(rewritten_title):
                print(f"   ‚ö†Ô∏è  –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            if not is_russian_text(rewritten_text):
                print(f"   ‚ö†Ô∏è  –¢–µ–∫—Å—Ç –Ω–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            if not has_hashtags(rewritten_text):
                print(f"   ‚ö†Ô∏è  –ù–µ—Ç —Ö—ç—à—Ç–µ–≥–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            if not is_telegram_compatible(rewritten_title, rewritten_text, link):
                print(f"   ‚ö†Ô∏è  –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç Telegram, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            print(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ")

            processed_news.append({
                "title": rewritten_title,
                "link": news.get("link", ""),
                "description": rewritten_text,
                "published": news.get("published", ""),
                "author": news.get("author", ""),
                "categories": news.get("categories", []),
                "image": news.get("image")
            })
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            continue

    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –≤ {OUTPUT_FILE}...")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(processed_news, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(processed_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

if __name__ == "__main__":
    main()
