#!/usr/bin/env python3
# process_ai.py ‚Äî —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∫—ç—à–µ–º, retry, fallback –∏ –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º –Ω–∞ —Å—Ç–∞—Ç—å—é

import os
import json
import time
import re
import random
from difflib import SequenceMatcher
from pathlib import Path
import requests
from bs4 import BeautifulSoup

# --- Google GenAI SDK Imports ---
from google import genai
from google.genai import types
# --------------------------------

# –ó–∞–≥—Ä—É–∑–∫–∞ .env —Ñ–∞–π–ª–∞ (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω)
def load_env_file():
    env_path = Path(__file__).parent.parent / '.env'
    if env_path.exists():
        with open(env_path, encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ.setdefault(key.strip(), value.strip())

load_env_file()

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª—é—á–∞
API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise Exception("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é GEMINI_API_KEY")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (–º–æ–∂–Ω–æ –ø–æ–¥–ø—Ä–∞–≤–∏—Ç—å)
DUPLICATE_THRESHOLD = 0.8
RUSSIAN_TEXT_THRESHOLD = 0.8
MAX_TELEGRAM_LENGTH = 4000
INPUT_FILE = "news_raw.json"

OUTPUT_FILE = "result_news.json"
REJECTED_FILE = "rejected_news.json"
IMAGES_DIR = "processed_images"
CACHE_FILE = "gemini_cache.json"

# Rate limiting / delays
GLOBAL_DELAY = float(os.getenv("GLOBAL_DELAY", "12"))  # —Å–µ–∫ –º–µ–∂–¥—É –≤—ã–∑–æ–≤–∞–º–∏ –∫ Gemini
BASE_RETRY_DELAY = 5  # –±–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ backoff
MAX_RETRIES = 5

# –ú–æ–¥–µ–ª–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ (fallback-ready)
MODEL_FALLBACKS = [
    "gemini-2.5-flash",
    "gemini-2.0-flash",
    "gemini-1.5-flash-001"
]

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
os.makedirs(IMAGES_DIR, exist_ok=True)

# --- –£—Ç–∏–ª–∏—Ç—ã ---
def is_duplicate(title, seen_titles):
    for seen in seen_titles:
        if SequenceMatcher(None, title.lower(), seen.lower()).ratio() > DUPLICATE_THRESHOLD:
            return True
    return False

def is_russian_text(text, threshold=RUSSIAN_TEXT_THRESHOLD):
    if not text or not text.strip():
        return False
    russian_chars = set('–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø')
    letters = [c for c in text if c.isalpha()]
    if not letters:
        return False
    russian_count = sum(1 for c in letters if c in russian_chars)
    return (russian_count / len(letters)) >= threshold

def has_hashtags(text):
    if not text or not text.strip():
        return False
    hashtags = re.findall(r'#\w+', text)
    return len(hashtags) >= 2

def is_telegram_compatible(title, description, link):
    formatted_text = f"üì∞ *{title}*\n\n{description}\n\nüîó [–°—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫]({link})"
    return len(formatted_text) <= MAX_TELEGRAM_LENGTH

def fetch_article_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
        }
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.content, 'html.parser')
        for el in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
            el.decompose()
        article_text = ""
        article = soup.find('article')
        if article:
            ps = article.find_all('p')
            article_text = ' '.join([p.get_text().strip() for p in ps if p.get_text().strip()])
        if not article_text:
            content_divs = soup.find_all(['div'], class_=lambda x: x and any(word in str(x).lower() for word in ['article','content','story','post']))
            for div in content_divs:
                ps = div.find_all('p')
                t = ' '.join([p.get_text().strip() for p in ps if p.get_text().strip()])
                if len(t) > len(article_text):
                    article_text = t
        if not article_text or len(article_text) < 200:
            ps = soup.find_all('p')
            article_text = ' '.join([p.get_text().strip() for p in ps if p.get_text().strip()])
        return article_text[:10000] if article_text else ""
    except Exception as e:
        print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
        return ""

def clean_ai_response(text):
    phrases_to_remove = [
        "–í–æ—Ç –∫—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞:", "–ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞:", "–í–æ—Ç –∫—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ:", "–ö—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ:",
        "–í–æ—Ç –ø–µ—Ä–µ–≤–æ–¥:", "–ü–µ—Ä–µ–≤–æ–¥:", "–í–æ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫:", "–ó–∞–≥–æ–ª–æ–≤–æ–∫:", "–í–æ—Ç –ø–µ—Ä–µ—Å–∫–∞–∑:", "–ü–µ—Ä–µ—Å–∫–∞–∑:",
        "–í–æ—Ç –Ω–æ–≤–æ—Å—Ç—å:", "–ù–æ–≤–æ—Å—Ç—å:"
    ]
    result = text.strip()
    for p in phrases_to_remove:
        if result.startswith(p):
            result = result[len(p):].strip()
    if (result.startswith('"') and result.endswith('"')) or (result.startswith("'") and result.endswith("'")):
        result = result[1:-1].strip()
    return result

# --- –ö—ç—à –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ ---
def load_cache():
    try:
        if Path(CACHE_FILE).exists():
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, dict):
                    return data
                print(f"   ‚ö†Ô∏è –ö—ç—à —Ñ–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥—ë–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π")
    except Exception as e:
        print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞: {e}")
    return {}

def save_cache(cache):
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à: {e}")

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ ---
client = genai.Client(api_key=API_KEY)

def parse_json_from_text(text):
    """
    –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON-–æ–±—ä–µ–∫—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None.
    """
    try:
        # –ü–æ–ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ {...} –±–ª–æ–∫
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            candidate = match.group(0)
            # —á–∏—Å—Ç–∏–º –Ω–µ–ø–µ—á–∞—Ç–∞–µ–º—ã–µ —Å–∏–º–≤–æ–ª—ã
            candidate = candidate.strip()
            return json.loads(candidate)
        # –ï—Å–ª–∏ –Ω–µ—Ç JSON –≤ —Ç–µ–∫—Å—Ç–µ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–æ–±—Ä–∞—Ç—å –ø—Ä–æ—Å—Ç—ã–º —Å–ø–æ—Å–æ–±–æ–º
        return None
    except Exception:
        return None

def gemini_request_single_json(article_text, max_retries=MAX_RETRIES, base_delay=BASE_RETRY_DELAY):
    """
    –î–µ–ª–∞–µ—Ç –æ–¥–∏–Ω (–ª–æ–≥–∏—á–µ—Å–∫–∏–π) –∑–∞–ø—Ä–æ—Å –∫ Gemini, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å JSON:
    { "title_ru": "...", "summary_ru": "...", "hashtags": ["#a","#b"] }
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict –∏–ª–∏ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç Exception.
    """
    # –ü—Ä–æ–º–ø—Ç: –ø—Ä–æ—Å–∏–º –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —á–∏—Å—Ç—ã–π JSON –≤ –∫–æ–¥–æ–≤–æ–º –±–ª–æ–∫–µ –∏–ª–∏ –±–µ–∑ –Ω–µ–≥–æ.
    prompt = (
        "–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞ –ø—Ä–æ –∂–∏–∑–Ω—å –≤ –ò—Å–ø–∞–Ω–∏–∏.\n\n"
        "1) –ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ —Å–æ–∑–¥–∞–π –ö–†–ê–¢–ö–ò–ô –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ —Ä—É—Å—Å–∫–æ–º (–¥–æ 10 —Å–ª–æ–≤) ‚Äî –ø–æ–ª–µ title_ru.\n"
        "2) –ü–∏—à–∏ –≤ —Å—Ç–∏–ª–µ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ù–µ–≤–∑–æ—Ä–æ–≤–∞, –Ω–æ –Ω–∞ 60% –æ—Ç –µ–≥–æ —Å—Ç–∏–ª—è.\n"
        "3) –°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º (5-6 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π; —Ä–∞–∑–¥–µ–ª—è–π –Ω–∞ –∞–±–∑–∞—Ü—ã, –µ—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ; –º–∞–∫—Å–∏–º—É–º 4000 —Å–∏–º–≤–æ–ª–æ–≤) ‚Äî –ø–æ–ª–µ summary_ru.\n"
        "4) –ü–æ–¥–±–µ—Ä–∏ 3-4 —Ö—ç—à—Ç–µ–≥–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é (–±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤, —Å #) ‚Äî –ø–æ–ª–µ hashtags (–º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫).\n\n"
        "–í–ï–†–ù–ò –°–¢–†–û–ì–û JSON –û–ë–™–ï–ö–¢ –° –ü–û–õ–Ø–ú–ò: title_ru, summary_ru, hashtags.\n"
        "–ü—Ä–∏–º–µ—Ä –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞:\n"
        '{"title_ru":"...","summary_ru":"...","hashtags":["#madrid","#immigration"]}\n\n'
        "–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏:\n\n" + article_text
    )

    cache = load_cache()
    cache_key = str(hash(prompt))
    if cache_key in cache:
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        return cache[cache_key]

    last_error = None
    for attempt in range(max_retries):
        model = MODEL_FALLBACKS[min(attempt, len(MODEL_FALLBACKS) - 1)]
        try:
            if attempt > 0:
                print(f"   üîÅ Retry {attempt}/{max_retries}, trying model={model}")
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            response = client.models.generate_content(
                model=model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    max_output_tokens=2100,
                    response_mime_type="application/json"
                )
            )
            text = ""
            if hasattr(response, "text") and response.text:
                text = response.text
            else:
                raise Exception("No text in response from model")
            text = clean_ai_response(text)

            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å JSON
            parsed = parse_json_from_text(text)
            if parsed and isinstance(parsed, dict):
                # –ù–µ–±–æ–ª—å—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
                title = parsed.get("title_ru", "").strip()
                summary = parsed.get("summary_ru", "").strip()
                hashtags = parsed.get("hashtags", [])
                if title and summary and isinstance(hashtags, list) and len(hashtags) >= 1:
                    result = {"title_ru": title, "summary_ru": summary, "hashtags": hashtags}
                    cache[cache_key] = result
                    save_cache(cache)
                    return result
                # –µ—Å–ª–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ ‚Äî –ø–æ–ø—ã—Ç–∞–µ–º—Å—è –¥–∞–ª—å—à–µ (–∫–∞—Å—Ç–æ–º)
            # –ï—Å–ª–∏ –Ω–µ JSON ‚Äî –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫/—Ö—ç—à—Ç–µ–≥–∏ –≤—Ä—É—á–Ω—É—é
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ö—ç—à—Ç–µ–≥–∏ –≤ –æ—Ç–≤–µ—Ç–µ
            found_tags = re.findall(r'#\w+', text)
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫-–ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ
            guessed_title = None
            # –ò–Ω–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç "title: ..." –≤ —Ç–µ–∫—Å—Ç–µ
            m = re.search(r'(?:title[:\-]\s*)(.+)', text, re.IGNORECASE)
            if m:
                guessed_title = m.group(1).split('\n')[0].strip()
            if not guessed_title:
                # –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                first_line = text.splitlines()[0].strip() if text.splitlines() else ""
                if 3 <= len(first_line.split()) <= 12:
                    guessed_title = first_line
            guessed_summary = text
            result = {
                "title_ru": guessed_title or "",
                "summary_ru": guessed_summary or text,
                "hashtags": found_tags or []
            }
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –ø–æ–ø—ã—Ç–æ–∫
            cache[cache_key] = result
            save_cache(cache)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è: –µ—Å–ª–∏ –ø—É—Å—Ç–æ–π title –∏–ª–∏ summary ‚Äî –±—Ä–æ—Å–∞–µ–º –æ—à–∏–±–∫—É, —á—Ç–æ–±—ã —Ä–µ—Ç—Ä–∞–∏—Ç—å/fallback
            if not result["title_ru"] or not result["summary_ru"]:
                raise Exception("Invalid/empty parsed response from model")

            return result

        except Exception as e:
            last_error = str(e)
            le = last_error.lower()
            print(f"   ‚ö†Ô∏è  Gemini error (model={model}): {last_error[:140]}")

            # –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
            overloaded = ("503" in le) or ("overload" in le) or ("unavailable" in le) or ("overloaded" in le)
            rate_limit = ("429" in le) or ("rate limit" in le) or ("quota" in le)
            timeout_err = ("timeout" in le) or ("timed out" in le)

            # exponential backoff + jitter
            if rate_limit:
                delay = 30 + attempt * 10 + random.uniform(0, 3)
                print(f"   üö´ Rate limit detected. Waiting {int(delay)}s before retry...")
                time.sleep(delay)
            elif overloaded:
                delay = base_delay * (2 ** attempt) + random.uniform(0, 2)
                print(f"   üßò Model overloaded. Waiting {int(delay)}s before retry (will fallback to next model if persists)...")
                time.sleep(delay)
            elif timeout_err:
                delay = base_delay * (attempt + 1) + random.uniform(0, 2)
                print(f"   ‚è± Timeout. Waiting {int(delay)}s before retry...")
                time.sleep(delay)
            else:
                delay = base_delay * (attempt + 1) + random.uniform(0, 2)
                print(f"   ‚è≥ Other error. Waiting {int(delay)}s before retry...")
                time.sleep(delay)
            # –ø–µ—Ä–µ–π–¥—ë–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –ø–æ–≤—Ç–æ—Ä—É (–≤–∫–ª—é—á–∞—è —Å–º–µ–Ω—É –º–æ–¥–µ–ª–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω–¥–µ–∫—Å–∞–º)

    # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
    raise Exception(f"Gemini failed after {max_retries} retries. Last error: {last_error}")

# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ---
def main():
    input_path = Path(__file__).parent.parent / INPUT_FILE
    output_path = Path(__file__).parent.parent / OUTPUT_FILE
    rejected_path = Path(__file__).parent.parent / REJECTED_FILE

    if not input_path.exists():
        print(f"‚ùå –§–∞–π–ª {INPUT_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    with open(input_path, 'r', encoding='utf-8') as f:
        news_items = json.load(f)

    if not news_items or not isinstance(news_items, list):
        print(f"‚ùå –§–∞–π–ª {INPUT_FILE} –ø—É—Å—Ç –∏–ª–∏ –∏–º–µ–µ—Ç –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.")
        return

    print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π")

    processed_news = []
    rejected_news = []
    seen_titles = []
    cache = load_cache()

    for idx, news in enumerate(news_items, start=1):
        title = news.get("title", "").strip()
        description = news.get("description", "").strip()
        link = news.get("link", "").strip()

        print(f"\n[{idx}/{len(news_items)}] {title[:80]}")

        if is_duplicate(title, seen_titles):
            print("   ‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            rejected_news.append({"title": title, "reason": "duplicate"})
            continue
        seen_titles.append(title)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ (—Å–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç—å—é)
        article_content = ""
        if link:
            print("   üîó –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç—å—é...")
            article_content = fetch_article_content(link)
            if article_content:
                print(f"   üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(article_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                print("   ‚ö†Ô∏è –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–µ –ø–æ–ª—É—á–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º description")

        text_for_model = (title + ". " + (article_content or description or title))[:12000]

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –≤—ã–∑–æ–≤–∞–º–∏ –∫ Gemini
        print(f"   üí§ –ñ–¥—ë–º {GLOBAL_DELAY}s –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º –∫ Gemini (–≥–ª–æ–±–∞–ª—å–Ω—ã–π rate limit)")
        time.sleep(GLOBAL_DELAY)

        try:
            ai_result = gemini_request_single_json(text_for_model)
        except Exception as e:
            print(f"   ‚ùå –ü—Ä–æ–±–ª–µ–º–∞ —Å Gemini: {e}")
            rejected_news.append({"title": title, "reason": f"gemini_error: {str(e)}"})
            continue

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –ø–æ–ª—è (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø—Ä–µ–∂–Ω–µ–º —Ñ–æ—Ä–º–∞—Ç–µ)
        rewritten_title = ai_result.get("title_ru", "").strip()
        rewritten_text = ai_result.get("summary_ru", "").strip()
        hashtags = ai_result.get("hashtags", [])

        # –í–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞–∫ —Ä–∞–Ω—å—à–µ
        if not rewritten_title:
            print("   ‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç –º–æ–¥–µ–ª–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            rejected_news.append({"title": title, "reason": "empty_title"})
            continue
        if not rewritten_text:
            print("   ‚ö†Ô∏è –ü—É—Å—Ç–æ–π summary –æ—Ç –º–æ–¥–µ–ª–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            rejected_news.append({"title": title, "reason": "empty_summary"})
            continue
        if not is_russian_text(rewritten_title):
            print("   ‚ö†Ô∏è –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º >=80%, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            rejected_news.append({"title": title, "reason": "not_russian_title"})
            continue
        if not is_russian_text(rewritten_text):
            print("   ‚ö†Ô∏è –¢–µ–∫—Å—Ç –Ω–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º >=80%, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            rejected_news.append({"title": title, "reason": "not_russian_text"})
            continue
        if not hashtags or len(hashtags) < 2:
            print("   ‚ö†Ô∏è –ú–∞–ª–æ —Ö—ç—à—Ç–µ–≥–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            rejected_news.append({"title": title, "reason": "few_hashtags"})
            continue
        # –î–æ–±–∞–≤–ª—è–µ–º —Ö—ç—à—Ç–µ–≥–∏ –≤ –∫–æ–Ω–µ—Ü summary, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        if not re.search(r'#\w+', rewritten_text):
            rewritten_text = rewritten_text.rstrip() + "\n\n" + " ".join(hashtags[:4])

        if not is_telegram_compatible(rewritten_title, rewritten_text, link):
            print("   ‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç Telegram, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            rejected_news.append({"title": title, "reason": "telegram_limit"})
            continue

        print(f"   ‚úÖ –û–ö: {rewritten_title[:60]} / summary {len(rewritten_text)} chars / tags {len(hashtags)}")

        processed_news.append({
            "title": rewritten_title,
            "link": link,
            "description": rewritten_text,
            "published": news.get("published", ""),
            "author": news.get("author", ""),
            "categories": news.get("categories", []),
            "image": news.get("image")
        })

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(processed_news)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –≤ {OUTPUT_FILE}...")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(processed_news, f, ensure_ascii=False, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–∫–ª–æ–Ω–µ–Ω–Ω—ã–µ
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(rejected_news)} –æ—Ç–∫–ª–æ–Ω–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –≤ {REJECTED_FILE}...")
    with open(rejected_path, 'w', encoding='utf-8') as f:
        json.dump(rejected_news, f, ensure_ascii=False, indent=2)

    print("‚úÖ –ì–æ—Ç–æ–≤–æ.")

if __name__ == "__main__":
    main()