import feedparser
import json
from dateutil import parser as dateparser
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup

# RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏
RSS_FEEDS = [
    "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada",
    "https://e00-elmundo.uecdn.es/elmundo/rss/espana.xml"
    # –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –µ—â—ë
]

def extract_image(entry):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–≤—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª–µ–π RSS item"""
    # 1Ô∏è‚É£ media:thumbnail
    if "media_thumbnail" in entry and len(entry.media_thumbnail) > 0:
        return entry.media_thumbnail[0].get("url")

    # 2Ô∏è‚É£ media:content (–µ—Å–ª–∏ —ç—Ç–æ –≤–∏–¥–µ–æ, —Ç–æ thumbnail –≤–Ω—É—Ç—Ä–∏)
    if "media_content" in entry and len(entry.media_content) > 0:
        for media in entry.media_content:
            if "medium" in media and media["medium"] == "image" and "url" in media:
                return media["url"]
            if "url" in media and "jpg" in media["url"]:
                return media["url"]

    # 3Ô∏è‚É£ enclosure (–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π RSS)
    if "enclosures" in entry and len(entry.enclosures) > 0:
        return entry.enclosures[0].get("url")

    # 4Ô∏è‚É£ –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏—â–µ–º <img>
    if "content" in entry and len(entry.content) > 0:
        html = entry.content[0].value
        soup = BeautifulSoup(html, "html.parser")
        img = soup.find("img")
        if img and img.get("src"):
            return img["src"]

    # 5Ô∏è‚É£ –≤ description
    if "description" in entry:
        soup = BeautifulSoup(entry.description, "html.parser")
        img = soup.find("img")
        if img and img.get("src"):
            return img["src"]

    return None


def fetch_recent_news(max_age_hours=3):
    now = datetime.now(timezone.utc)
    news_items = []

    for feed_url in RSS_FEEDS:
        print(f"üîπ Fetching: {feed_url}")
        feed = feedparser.parse(feed_url)

        for entry in feed.entries:
            # –ø–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É
            pub_date = None
            if hasattr(entry, "published"):
                try:
                    pub_date = dateparser.parse(entry.published)
                except Exception:
                    pass
            elif hasattr(entry, "updated"):
                try:
                    pub_date = dateparser.parse(entry.updated)
                except Exception:
                    pass

            # –µ—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if not pub_date:
                continue

            # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            if (now - pub_date) > timedelta(hours=max_age_hours):
                continue

            news = {
                "title": entry.title.strip() if hasattr(entry, "title") else "",
                "link": entry.link if hasattr(entry, "link") else "",
                "description": entry.get("description", "").strip(),
                "published": pub_date.isoformat(),
                "author": entry.get("author", ""),
                "categories": entry.get("tags", []),
                "image": extract_image(entry),
            }
            news_items.append(news)

    return news_items


def remove_duplicates(new_news, existing_news):
    """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π, –ø—Ä–æ–≤–µ—Ä—è—è –ø–æ —Å—Å—ã–ª–∫–µ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª"""
    # –°–æ–∑–¥–∞—ë–º set –∏–∑ —Å—Å—ã–ª–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    existing_links = {item['link'] for item in existing_news if 'link' in item}

    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    unique_news = []
    duplicates_count = 0

    for news in new_news:
        if news['link'] not in existing_links:
            unique_news.append(news)
        else:
            duplicates_count += 1

    return unique_news, duplicates_count


if __name__ == "__main__":
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ —Ñ–∞–π–ª–∞
    existing_news = []
    try:
        with open("news_raw.json", "r", encoding="utf-8") as f:
            existing_news = json.load(f)
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(existing_news)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ news_raw.json")
    except FileNotFoundError:
        print("üìÇ –§–∞–π–ª news_raw.json –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π")
    except json.JSONDecodeError:
        print("‚ö†Ô∏è  –§–∞–π–ª news_raw.json –ø–æ–≤—Ä–µ–∂–¥—ë–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π")

    # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    news = fetch_recent_news()
    print(f"\nüì∞ –ü–æ–ª—É—á–µ–Ω–æ {len(news)} —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS\n")

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_news, duplicates = remove_duplicates(news, existing_news)

    if duplicates > 0:
        print(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–æ {duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")

    print(f"‚ú® –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(unique_news)}\n")

    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–æ–≤–æ—Å—Ç—è—Ö
    for n in unique_news:
        print(f"üß© {n['title']}")
        print(f"   üïí {n['published']}")
        print(f"   üîó {n['link']}")
        print(f"   ‚úçÔ∏è  {n['author']}")
        print(f"   üñºÔ∏è  {n['image']}")
        print(f"   üóÇÔ∏è  {[tag['term'] for tag in n['categories']] if n['categories'] else []}")
        print()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    with open("news_raw.json", "w", encoding="utf-8") as f:
        json.dump(unique_news, f, ensure_ascii=False, indent=2)

    print("‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ news_raw.json ‚Äî —Ç–µ–ø–µ—Ä—å –∑–∞–ø—É—Å—Ç–∏ process_ai.py")