import feedparser
import json
from dateutil import parser as dateparser
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup

# RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏
RSS_FEEDS = [
    "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada",
    "https://e00-elmundo.uecdn.es/elmundo/rss/espana.xml"
    # –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –µ—â—ë
]

def extract_image(entry):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–≤—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª–µ–π RSS item"""
    # 1Ô∏è‚É£ media:thumbnail
    if "media_thumbnail" in entry and len(entry.media_thumbnail) > 0:
        return entry.media_thumbnail[0].get("url")

    # 2Ô∏è‚É£ media:content (–µ—Å–ª–∏ —ç—Ç–æ –≤–∏–¥–µ–æ, —Ç–æ thumbnail –≤–Ω—É—Ç—Ä–∏)
    if "media_content" in entry and len(entry.media_content) > 0:
        for media in entry.media_content:
            if "medium" in media and media["medium"] == "image" and "url" in media:
                return media["url"]
            if "url" in media and "jpg" in media["url"]:
                return media["url"]

    # 3Ô∏è‚É£ enclosure (–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π RSS)
    if "enclosures" in entry and len(entry.enclosures) > 0:
        return entry.enclosures[0].get("url")

    # 4Ô∏è‚É£ –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏—â–µ–º <img>
    if "content" in entry and len(entry.content) > 0:
        html = entry.content[0].value
        soup = BeautifulSoup(html, "html.parser")
        img = soup.find("img")
        if img and img.get("src"):
            return img["src"]

    # 5Ô∏è‚É£ –≤ description
    if "description" in entry:
        soup = BeautifulSoup(entry.description, "html.parser")
        img = soup.find("img")
        if img and img.get("src"):
            return img["src"]

    return None


def is_spain_related(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–≤—è–∑–∞–Ω–∞ –ª–∏ –Ω–æ–≤–æ—Å—Ç—å —Å –ò—Å–ø–∞–Ω–∏–µ–π"""
    text_lower = text.lower()
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ò—Å–ø–∞–Ω–∏–µ–π
    spain_keywords = [
        'espa√±a', 'espa√±ol', 'espa√±ola', 'espa√±oles', 'espa√±olas',
        'madrid', 'barcelona', 'valencia', 'sevilla', 'zaragoza', 'm√°laga',
        'catalu√±a', 'andaluc√≠a', 'pa√≠s vasco', 'galicia', 'castilla',
        'gobierno espa√±ol', 'rey felipe', 'pedro s√°nchez', 'pp', 'psoe', 'vox',
        'congreso de los diputados', 'senado espa√±ol',
        'espa√±'  # –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö —Å–ª–æ–≤
    ]
    
    return any(keyword in text_lower for keyword in spain_keywords)


def is_not_advertisement(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –Ω–æ–≤–æ—Å—Ç—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–∫–ª–∞–º–æ–π (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    text_lower = text.lower()
    
    # –°–∏–ª—å–Ω—ã–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã (–µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω - —Ç–æ—á–Ω–æ —Ä–µ–∫–ª–∞–º–∞)
    strong_ad_keywords = [
        'comprar ahora', 'compra ahora', 'c√≥mpralo',
        'haz clic aqu√≠', 'pincha aqu√≠',
        'solicita', 'solicitud gratuita',
        'llama ahora', 'contacta ahora',
        'visita nuestra tienda',
        'a√±adir al carrito',
        'pagar ahora',
    ]
    
    # –°–ª–∞–±—ã–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã (–Ω—É–∂–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ)
    weak_ad_keywords = [
        'oferta', 'descuento', 'promoci√≥n', 'rebaja',
        'precio especial', 'precio reducido',
        'ahorra', '% de descuento', 'gratis',
        'patrocinado', 'publicidad', 'anuncio',
        'suscr√≠bete', 'suscripci√≥n', 'prueba gratis',
        'hasta agotar', 'por tiempo limitado',
        'liquidaci√≥n', 'outlet', 'chollazo',
    ]
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–∏–ª—å–Ω—ã–π –º–∞—Ä–∫–µ—Ä - —ç—Ç–æ —Ä–µ–∫–ª–∞–º–∞
    if any(keyword in text_lower for keyword in strong_ad_keywords):
        return False
    
    # –°—á–∏—Ç–∞–µ–º —Å–ª–∞–±—ã–µ –º–∞—Ä–∫–µ—Ä—ã
    weak_count = sum(1 for keyword in weak_ad_keywords if keyword in text_lower)
    
    # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 2 —Å–ª–∞–±—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ - —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —Ä–µ–∫–ª–∞–º–∞
    return weak_count < 3


def is_valid_news(news_item):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å –≤–∞–ª–∏–¥–Ω–æ–π (–ø—Ä–æ –ò—Å–ø–∞–Ω–∏—é –∏ –Ω–µ —Ä–µ–∫–ª–∞–º–∞)"""
    title = news_item.get('title', '')
    description = news_item.get('description', '')
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    full_text = f"{title} {description}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–∞ —É—Å–ª–æ–≤–∏—è
    spain_related = is_spain_related(full_text)
    not_ad = is_not_advertisement(full_text)
    
    return spain_related and not_ad


def fetch_recent_news(max_age_hours=2):
    now = datetime.now(timezone.utc)
    news_items = []

    for feed_url in RSS_FEEDS:
        print(f"üîπ Fetching: {feed_url}")
        feed = feedparser.parse(feed_url)

        for entry in feed.entries:
            # –ø–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É
            pub_date = None
            if hasattr(entry, "published"):
                try:
                    pub_date = dateparser.parse(entry.published)
                except Exception:
                    pass
            elif hasattr(entry, "updated"):
                try:
                    pub_date = dateparser.parse(entry.updated)
                except Exception:
                    pass

            # –µ—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if not pub_date:
                continue

            # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            if (now - pub_date) > timedelta(hours=max_age_hours):
                continue

            news = {
                "title": entry.title.strip() if hasattr(entry, "title") else "",
                "link": entry.link if hasattr(entry, "link") else "",
                "description": entry.get("description", "").strip(),
                "published": pub_date.isoformat(),
                "author": entry.get("author", ""),
                "categories": entry.get("tags", []),
                "image": extract_image(entry),
            }
            news_items.append(news)

    return news_items


def remove_duplicates(new_news, existing_news):
    """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π, –ø—Ä–æ–≤–µ—Ä—è—è –ø–æ —Å—Å—ã–ª–∫–µ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª"""
    # –°–æ–∑–¥–∞—ë–º set –∏–∑ —Å—Å—ã–ª–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    existing_links = {item['link'] for item in existing_news if 'link' in item}

    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    unique_news = []
    duplicates_count = 0

    for news in new_news:
        if news['link'] not in existing_links:
            unique_news.append(news)
        else:
            duplicates_count += 1

    return unique_news, duplicates_count


if __name__ == "__main__":
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ —Ñ–∞–π–ª–∞
    existing_news = []
    try:
        with open("news_raw.json", "r", encoding="utf-8") as f:
            existing_news = json.load(f)
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(existing_news)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ news_raw.json")
    except FileNotFoundError:
        print("üìÇ –§–∞–π–ª news_raw.json –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π")
    except json.JSONDecodeError:
        print("‚ö†Ô∏è  –§–∞–π–ª news_raw.json –ø–æ–≤—Ä–µ–∂–¥—ë–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π")

    # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    news = fetch_recent_news()
    print(f"\nüì∞ –ü–æ–ª—É—á–µ–Ω–æ {len(news)} —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS\n")

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_news, duplicates = remove_duplicates(news, existing_news)

    if duplicates > 0:
        print(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–æ {duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")

    print(f"‚ú® –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(unique_news)}\n")

    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏: –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ –ò—Å–ø–∞–Ω–∏—é –∏ –Ω–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ
    filtered_news = []
    rejected_count = 0
    rejected_reasons = {
        'not_spain': 0,
        'advertisement': 0,
        'both': 0
    }

    for news_item in unique_news:
        spain_related = is_spain_related(f"{news_item['title']} {news_item['description']}")
        not_ad = is_not_advertisement(f"{news_item['title']} {news_item['description']}")
        
        if spain_related and not_ad:
            filtered_news.append(news_item)
        else:
            rejected_count += 1
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏—á–∏–Ω—É –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            if not spain_related and not not_ad:
                rejected_reasons['both'] += 1
                reason = "–Ω–µ –ø—Ä–æ –ò—Å–ø–∞–Ω–∏—é + —Ä–µ–∫–ª–∞–º–∞"
            elif not spain_related:
                rejected_reasons['not_spain'] += 1
                reason = "–Ω–µ –ø—Ä–æ –ò—Å–ø–∞–Ω–∏—é"
            else:
                rejected_reasons['advertisement'] += 1
                reason = "—Ä–µ–∫–ª–∞–º–∞"
            
            print(f"‚ùå –û—Ç–∫–ª–æ–Ω–µ–Ω–æ ({reason}): {news_item['title'][:60]}...")

    print(f"\nüö´ –û—Ç–∫–ª–æ–Ω–µ–Ω–æ {rejected_count} –Ω–æ–≤–æ—Å—Ç–µ–π:")
    print(f"   üìç –ù–µ –ø—Ä–æ –ò—Å–ø–∞–Ω–∏—é: {rejected_reasons['not_spain']}")
    print(f"   üõí –†–µ–∫–ª–∞–º–∞: {rejected_reasons['advertisement']}")
    print(f"   ‚ö†Ô∏è  –û–±–∞ –∫—Ä–∏—Ç–µ—Ä–∏—è: {rejected_reasons['both']}")
    print(f"‚úÖ –ü—Ä–æ—à–ª–æ –ø—Ä–æ–≤–µ—Ä–∫—É: {len(filtered_news)} –Ω–æ–≤–æ—Å—Ç–µ–π\n")

    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–æ–≤–æ—Å—Ç—è—Ö
    for n in filtered_news:
        print(f"üß© {n['title']}")
        print(f"   üïí {n['published']}")
        print(f"   üîó {n['link']}")
        print(f"   ‚úçÔ∏è  {n['author']}")
        print(f"   üñºÔ∏è  {n['image']}")
        print(f"   üóÇÔ∏è  {[tag['term'] for tag in n['categories']] if n['categories'] else []}")
        print()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—å —Ñ–∞–π–ª–∞)
    with open("news_raw.json", "w", encoding="utf-8") as f:
        json.dump(filtered_news, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(filtered_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ news_raw.json")