import feedparser
import json
from dateutil import parser as dateparser
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
from url_tracker import URLTracker

# RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏
RSS_FEEDS = [
    "https://e00-elmundo.uecdn.es/elmundo/rss/espana.xml",
    "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada"
    # –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –µ—â—ë
]

def extract_image(entry):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–≤—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª–µ–π RSS item"""
    # 1Ô∏è‚É£ media:thumbnail
    if "media_thumbnail" in entry and len(entry.media_thumbnail) > 0:
        return entry.media_thumbnail[0].get("url")

    # 2Ô∏è‚É£ media:content (–µ—Å–ª–∏ —ç—Ç–æ –≤–∏–¥–µ–æ, —Ç–æ thumbnail –≤–Ω—É—Ç—Ä–∏)
    if "media_content" in entry and len(entry.media_content) > 0:
        for media in entry.media_content:
            if "medium" in media and media["medium"] == "image" and "url" in media:
                return media["url"]
            if "url" in media and "jpg" in media["url"]:
                return media["url"]

    # 3Ô∏è‚É£ enclosure (–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π RSS)
    if "enclosures" in entry and len(entry.enclosures) > 0:
        return entry.enclosures[0].get("url")

    # 4Ô∏è‚É£ –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏—â–µ–º <img>
    if "content" in entry and len(entry.content) > 0:
        html = entry.content[0].value
        soup = BeautifulSoup(html, "html.parser")
        img = soup.find("img")
        if img and img.get("src"):
            return img["src"]

    # 5Ô∏è‚É£ –≤ description
    if "description" in entry:
        soup = BeautifulSoup(entry.description, "html.parser")
        img = soup.find("img")
        if img and img.get("src"):
            return img["src"]

    return None


def is_spain_related(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–≤—è–∑–∞–Ω–∞ –ª–∏ –Ω–æ–≤–æ—Å—Ç—å —Å –ò—Å–ø–∞–Ω–∏–µ–π"""
    text_lower = text.lower()
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ò—Å–ø–∞–Ω–∏–µ–π
    spain_keywords = [
         # —Å—Ç—Ä–∞–Ω—ã/–≥–æ—Ä–æ–¥–∞
        "espa√±", "espa√±a", "madrid", 'val√©ncia', "barcelona", "valencia", "sevilla", "zaragoza", "bilbao",
        "andaluc√≠a", "catalu√±a", "galicia", "pais vasco", "comunidad valenciana",
        "castilla", "navarra", "murcia", "asturias", "cantabria",

        # –æ—Ä–≥–∞–Ω—ã –≤–ª–∞—Å—Ç–∏
        "gobierno", "ayuntamiento", "comunidad aut√≥noma",
        "polic√≠a nacional", "guardia civil", "tribunal supremo",
        "audiencia nacional", "seguridad social", "hacienda",

        # –∏—Å–ø–∞–Ω—Å–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
        "bbva", "santander", "caixabank", "iberdrola", "repsol", "endesa",

        # –ø–æ–ª–∏—Ç–∏–∫–∞
        "psoe", "pp", "vox", "sumar", "podemos", "erc", "junts",
        "s√°nchez", "ayuso", "feij√≥o", "abascal", "yolanda d√≠az",
    ]
    
    return any(keyword in text_lower for keyword in spain_keywords)


def is_not_advertisement(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –Ω–æ–≤–æ—Å—Ç—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–∫–ª–∞–º–æ–π (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    text_lower = text.lower()
    
    # –°–∏–ª—å–Ω—ã–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã (–µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω - —Ç–æ—á–Ω–æ —Ä–µ–∫–ª–∞–º–∞)
    strong_ad_keywords = [
        'comprar ahora', 'compra ahora', 'c√≥mpralo',
        'haz clic aqu√≠', 'pincha aqu√≠',
        'solicita', 'solicitud gratuita',
        'llama ahora', 'contacta ahora',
        'visita nuestra tienda',
        'a√±adir al carrito',
        'pagar ahora',
    ]
    
    # –°–ª–∞–±—ã–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã (–Ω—É–∂–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ)
    weak_ad_keywords = [
        'oferta', 'descuento', 'promoci√≥n', 'rebaja',
        'precio especial', 'precio reducido',
        'ahorra', '% de descuento', 'gratis',
        'patrocinado', 'publicidad', 'anuncio',
        'suscr√≠bete', 'suscripci√≥n', 'prueba gratis',
        'hasta agotar', 'por tiempo limitado',
        'liquidaci√≥n', 'outlet', 'chollazo',
    ]
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–∏–ª—å–Ω—ã–π –º–∞—Ä–∫–µ—Ä - —ç—Ç–æ —Ä–µ–∫–ª–∞–º–∞
    if any(keyword in text_lower for keyword in strong_ad_keywords):
        return False
    
    # –°—á–∏—Ç–∞–µ–º —Å–ª–∞–±—ã–µ –º–∞—Ä–∫–µ—Ä—ã
    weak_count = sum(1 for keyword in weak_ad_keywords if keyword in text_lower)
    
    # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 2 —Å–ª–∞–±—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ - —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —Ä–µ–∫–ª–∞–º–∞
    return weak_count < 3


def is_valid_news(news_item):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å –≤–∞–ª–∏–¥–Ω–æ–π (–ø—Ä–æ –ò—Å–ø–∞–Ω–∏—é –∏ –Ω–µ —Ä–µ–∫–ª–∞–º–∞)"""
    title = news_item.get('title', '')
    description = news_item.get('description', '')
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    full_text = f"{title} {description}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–∞ —É—Å–ª–æ–≤–∏—è
    spain_related = is_spain_related(full_text)
    not_ad = is_not_advertisement(full_text)
    
    return spain_related and not_ad


def fetch_recent_news(max_age_hours=2):
    now = datetime.now(timezone.utc)
    news_items = []

    for feed_url in RSS_FEEDS:
        print(f"üîπ Fetching: {feed_url}")
        feed = feedparser.parse(feed_url)

        for entry in feed.entries:
            # –ø–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É
            pub_date = None
            if hasattr(entry, "published"):
                try:
                    pub_date = dateparser.parse(entry.published)
                except Exception:
                    pass
            elif hasattr(entry, "updated"):
                try:
                    pub_date = dateparser.parse(entry.updated)
                except Exception:
                    pass

            # –µ—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if not pub_date:
                continue

            # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            if (now - pub_date) > timedelta(hours=max_age_hours):
                continue

            news = {
                "title": entry.title.strip() if hasattr(entry, "title") else "",
                "link": entry.link if hasattr(entry, "link") else "",
                "description": entry.get("description", "").strip(),
                "published": pub_date.isoformat(),
                "author": entry.get("author", ""),
                "categories": entry.get("tags", []),
                "image": extract_image(entry),
            }
            news_items.append(news)

    return news_items





if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–∫–µ—Ä URL
    url_tracker = URLTracker()
    
    # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ URL (—Å—Ç–∞—Ä—à–µ 24 —á–∞—Å–æ–≤)
    removed_count = url_tracker.cleanup_old_urls()
    if removed_count > 0:
        print(f"üßπ –û—á–∏—â–µ–Ω–æ {removed_count} —Å—Ç–∞—Ä—ã—Ö URL –∏–∑ –±–∞–∑—ã (>24—á)\n")
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    news = fetch_recent_news()
    print(f"\nüì∞ –ü–æ–ª—É—á–µ–Ω–æ {len(news)} —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS\n")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã —á–µ—Ä–µ–∑ URL —Ç—Ä–µ–∫–µ—Ä
    unique_news = []
    duplicates_count = 0
    new_urls_to_track = []
    
    for news_item in news:
        url = news_item.get('link', '')
        if url and url_tracker.is_duplicate(url):
            duplicates_count += 1
        else:
            unique_news.append(news_item)
            # –°—Ä–∞–∑—É –¥–æ–±–∞–≤–ª—è–µ–º URL –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
            if url:
                new_urls_to_track.append(url)
    
    # –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º URL –≤ —Ç—Ä–µ–∫–µ—Ä, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞—Ö
    if new_urls_to_track:
        added_urls = url_tracker.add_urls_batch(new_urls_to_track)
        print(f"üíæ –î–æ–±–∞–≤–ª–µ–Ω–æ {added_urls} –Ω–æ–≤—ã—Ö URL –≤ –±–∞–∑—É –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è (–∑–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)")
    
    if duplicates_count > 0:
        print(f"üóëÔ∏è  –û—Ç–∫–ª–æ–Ω–µ–Ω–æ {duplicates_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (URL —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã)")
    
    print(f"‚ú® –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(unique_news)}\n")

    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏: –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ –ò—Å–ø–∞–Ω–∏—é –∏ –Ω–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ
    filtered_news = []
    rejected_count = 0
    rejected_reasons = {
        'not_spain': 0,
        'advertisement': 0,
        'both': 0
    }

    for news_item in unique_news:
        spain_related = is_spain_related(f"{news_item['title']} {news_item['description']}")
        not_ad = is_not_advertisement(f"{news_item['title']} {news_item['description']}")
        
        if spain_related and not_ad:
            filtered_news.append(news_item)
        else:
            rejected_count += 1
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏—á–∏–Ω—É –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            if not spain_related and not not_ad:
                rejected_reasons['both'] += 1
                reason = "–Ω–µ –ø—Ä–æ –ò—Å–ø–∞–Ω–∏—é + —Ä–µ–∫–ª–∞–º–∞"
            elif not spain_related:
                rejected_reasons['not_spain'] += 1
                reason = "–Ω–µ –ø—Ä–æ –ò—Å–ø–∞–Ω–∏—é"
            else:
                rejected_reasons['advertisement'] += 1
                reason = "—Ä–µ–∫–ª–∞–º–∞"
            
            print(f"‚ùå –û—Ç–∫–ª–æ–Ω–µ–Ω–æ ({reason}): {news_item['title'][:60]}...")

    print(f"\nüö´ –û—Ç–∫–ª–æ–Ω–µ–Ω–æ {rejected_count} –Ω–æ–≤–æ—Å—Ç–µ–π:")
    print(f"   üìç –ù–µ –ø—Ä–æ –ò—Å–ø–∞–Ω–∏—é: {rejected_reasons['not_spain']}")
    print(f"   üõí –†–µ–∫–ª–∞–º–∞: {rejected_reasons['advertisement']}")
    print(f"   ‚ö†Ô∏è  –û–±–∞ –∫—Ä–∏—Ç–µ—Ä–∏—è: {rejected_reasons['both']}")
    print(f"‚úÖ –ü—Ä–æ—à–ª–æ –ø—Ä–æ–≤–µ—Ä–∫—É: {len(filtered_news)} –Ω–æ–≤–æ—Å—Ç–µ–π\n")
    
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–æ–≤–æ—Å—Ç—è—Ö
    for n in filtered_news:
        print(f"üß© {n['title']}")
        print(f"   üïí {n['published']}")
        print(f"   üîó {n['link']}")
        print(f"   ‚úçÔ∏è  {n['author']}")
        print(f"   üñºÔ∏è  {n['image']}")
        print(f"   üóÇÔ∏è  {[tag['term'] for tag in n['categories']] if n['categories'] else []}")
        print()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—å —Ñ–∞–π–ª–∞)
    with open("news_raw.json", "w", encoding="utf-8") as f:
        json.dump(filtered_news, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(filtered_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ news_raw.json")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = url_tracker.get_stats()
    print(f"üìä –í—Å–µ–≥–æ URL –≤ –±–∞–∑–µ: {stats['total_urls']}")