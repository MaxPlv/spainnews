"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
"""
import json
import os
from datetime import datetime, timedelta
from pathlib import Path
from difflib import SequenceMatcher


PROJECT_ROOT = Path(__file__).parent.parent
PUBLISHED_NEWS_FILE = PROJECT_ROOT / "published_news.json"
HISTORY_DAYS = 14  # –•—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 14 –¥–Ω–µ–π


def similarity(text1: str, text2: str) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–æ—Ç 0.0 –¥–æ 1.0)
    """
    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()


def load_published_news() -> list:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    """
    if not PUBLISHED_NEWS_FILE.exists():
        return []
    
    try:
        with open(PUBLISHED_NEWS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ published_news.json: {e}")
        return []


def save_published_news(news_list: list):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    """
    try:
        with open(PUBLISHED_NEWS_FILE, 'w', encoding='utf-8') as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ published_news.json: {e}")


def cleanup_old_entries(news_list: list) -> list:
    """
    –£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å—Ç–∞—Ä—à–µ HISTORY_DAYS –¥–Ω–µ–π
    
    Args:
        news_list: –°–ø–∏—Å–æ–∫ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        
    Returns:
        –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
    """
    cutoff_date = datetime.now() - timedelta(days=HISTORY_DAYS)
    
    filtered = []
    for news in news_list:
        try:
            published_at = datetime.fromisoformat(news.get('published_at', ''))
            if published_at >= cutoff_date:
                filtered.append(news)
        except (ValueError, TypeError):
            # –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø–∏—Å—å
            continue
    
    removed_count = len(news_list) - len(filtered)
    if removed_count > 0:
        print(f"üßπ –£–¥–∞–ª–µ–Ω–æ {removed_count} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π (>{HISTORY_DAYS} –¥–Ω–µ–π)")
    
    return filtered


def add_published_news(title: str, text: str, url: str = ""):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –≤ –∏—Å—Ç–æ—Ä–∏—é –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö
    
    Args:
        title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏
        text: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
        url: URL –Ω–æ–≤–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    news_list = load_published_news()
    
    # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –Ω–æ–≤–æ–π
    news_list = cleanup_old_entries(news_list)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
    news_entry = {
        "title": title,
        "text": text,
        "url": url,
        "published_at": datetime.now().isoformat()
    }
    
    news_list.append(news_entry)
    save_published_news(news_list)
    print(f"‚úÖ –ù–æ–≤–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é –ø—É–±–ª–∏–∫–∞—Ü–∏–π (–≤—Å–µ–≥–æ: {len(news_list)})")


def check_duplicate(title: str, text: str, similarity_threshold: float = 0.85) -> dict:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–æ–º —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω–æ–π
    
    Args:
        title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–π –Ω–æ–≤–æ—Å—Ç–∏
        text: –¢–µ–∫—Å—Ç –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–π –Ω–æ–≤–æ—Å—Ç–∏
        similarity_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.0-1.0)
        
    Returns:
        dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏:
        - is_duplicate: bool
        - match: dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –Ω–∞–π–¥–µ–Ω–Ω–æ–º –¥—É–±–ª–∏–∫–∞—Ç–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        - similarity_score: float —Å—Ö–æ–∂–µ—Å—Ç—å —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º –¥—É–±–ª–∏–∫–∞—Ç–æ–º
    """
    news_list = load_published_news()
    
    # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
    news_list = cleanup_old_entries(news_list)
    
    for published_news in news_list:
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        title_sim = similarity(title, published_news.get('title', ''))
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã
        text_sim = similarity(text, published_news.get('text', ''))
        
        # –ë–µ—Ä—ë–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Å—Ö–æ–∂–µ—Å—Ç—å
        max_sim = max(title_sim, text_sim)
        
        if max_sim >= similarity_threshold:
            return {
                "is_duplicate": True,
                "match": published_news,
                "similarity_score": max_sim,
                "matched_by": "title" if title_sim > text_sim else "text"
            }
    
    return {
        "is_duplicate": False,
        "match": None,
        "similarity_score": 0.0
    }


def get_stats() -> dict:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–º –Ω–æ–≤–æ—Å—Ç—è–º
    """
    news_list = load_published_news()
    news_list = cleanup_old_entries(news_list)
    
    if not news_list:
        return {
            "total": 0,
            "oldest_date": None,
            "newest_date": None
        }
    
    dates = []
    for news in news_list:
        try:
            dates.append(datetime.fromisoformat(news.get('published_at', '')))
        except (ValueError, TypeError):
            continue
    
    return {
        "total": len(news_list),
        "oldest_date": min(dates).isoformat() if dates else None,
        "newest_date": max(dates).isoformat() if dates else None,
        "history_days": HISTORY_DAYS
    }
